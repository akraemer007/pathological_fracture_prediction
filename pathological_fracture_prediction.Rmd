---
title: "Pathological Fracture Prediction"
output: html_notebook
---

pathological_fracture_prediction

```{r packages}
library(caret)
library(tidyverse)
library(dplyr)
library(magrittr)
library(stringr)
# devtools::install_github('kidman007/kraemR')
library(kraemR)
library(hrbrthemes)
```

Download file from: [https://drive.google.com/open?id=0B4W_zSMkDcS9SlRrTXZlLVBGU3M](https://drive.google.com/open?id=0B4W_zSMkDcS9SlRrTXZlLVBGU3M) and save in your working directory.

```{r data_import}
# I'll figure this out later if I have time
# if (!file.exists("PathologicalFracturesDataSet.csv")) {
#   id <- "0B4W_zSMkDcS9SlRrTXZlLVBGU3M" # google file ID
#   read_csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# }

df_raw <- read_csv("PathologicalFracturesDataSet.csv")

```

```{r}
head(df_raw) %>% 
  View
```


```{r}
glimpse(df_raw)
```

```{r, include=FALSE}
row_count_no_na <- nrow(drop_na(df_raw))
```

There are `r nrow(df_raw) - row_count_no_na` rows with na values.

Which columns have NAs?
```{r}
df_raw %>% 
  map_lgl(anyNA) %>% # loop all columns with an anyNA check
  .[. == TRUE]
```
Looks like just age and Calcium.

```{r}
paste(nrow(df_raw[is.na(df_raw$age), ]), 'age records are NA')
paste(nrow(df_raw[is.na(df_raw$Calcium), ]), 'Calcium records are NA')
```

Are there any duplicate visit IDs?

```{r}
df_raw %>% 
  count(visitId) %>% 
  filter(n > 1) %>% 
  nrow %>% 
  paste(., 'duplicate visitIDs')
```


What numbers are in the variables? Are they all zeroes and ones? I'm removing the fields I can already tell are not zeroes and ones. 

```{r}
non_binary_vars <- c('visitId','gender', 'age', 'result', 'xrayTestService', 'petScanService', 'ctTestService', 'Calcium')

max_cols <- df_raw %>% 
  select(-visitId,
         -age,
         -result,
         -xrayTestService,
         -petScanService,
         -ctTestService,
         -Calcium) %>% 
  summarise_all(max) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column() %>% 
  rename(max_value = V1)

max_cols %>% 
  arrange(desc(max_value))

binary_vars <- max_cols %>% 
  filter(max_value %in% c(0,1)) %>% 
  pull(rowname)

```

As I expected, most of the variables are binary. It's probably worth noting that over 50 of the binary variables have no records that are a `1`. 

I created the variables `binary_vars` and `non_binary_vars` for later reference

Okay, interesting, apparantly there's a two in gender. Let's find out how many of those there are.

```{r}
df_raw %>% 
  count_pct(gender) %>% 
  select(-pct) %>% 
  arrange(gender)
```
Okay, it looks like there are only 22 cases of `gender = 2`. I'm not sure what I want to do with that.

Can there numbers in multiple columns? So I start with the first variable set: `C1882062`, which is apparantly "Neoplastic Syndrome."

```{r}
df_raw %>% 
  select(starts_with('C1882062')) %>%
  mutate(var_sum = C1882062 + C1882062Historic + C1882062Negated + C1882062Uncertain + C1882062Surgical) %>% 
  # filter(var_sum > 0 ) %>% 
  arrange(desc(var_sum)) %>% 
  head

# 
df_raw %>%
  select(starts_with('C1882062')) %>%
  mutate(var_sum = C1882062 + C1882062Historic + C1882062Negated + C1882062Uncertain + C1882062Surgical) %>%
  filter(
         # C1882062 == 0,
         var_sum > 0) %>%
  arrange(desc(var_sum)) %>%
  count_pct(C1882062)

```

Okay, this is interesting there are no examples for `C1882062` that have all 5. Let me check this across all of the non-binary variables.

```{r}

suffixes <- c('Historic', 'Negated', 'Uncertain', 'Surgical')

length(binary_vars) / 5 # looks like there are the correct number of columns
  
# get unique binary vars 
binary_vars_unique <- binary_vars %>% 
  str_replace('Historic', '') %>% 
  str_replace('Negated', '') %>% 
  str_replace('Uncertain', '') %>% 
  str_replace('Surgical', '') %>% 
  unique

# get the max number of `1s` a variable can have across the different types
var_max_answer_check <- function(var, df = df_raw) {
  df %>% 
  select(starts_with(var)) %>% 
  rowSums %>% 
  max
}

# loop by different variables
map_dbl(binary_vars_unique, var_max_answer_check) %>% 
  as_tibble() %>% 
  count(value) %>% 
  mutate(pct = round(n/ sum(n),4))

map_dbl(binary_vars_unique, var_max_answer_check, df = filter(df_raw, result == 'yes')) %>% 
  as_tibble() %>% 
  count(value) %>% 
  mutate(pct = round(n/ sum(n),4))
```

Still trying to understand these variables. They can have 5 that are coded 1, or they can have things that are coded without the "vanilla" version of the variable being coded.



## Cleaning data
Dealing with NAs

First, let's take a look the age distribution before we simply take the median.
```{r}
df_raw %>% 
  ggplot(aes(age)) +
    geom_histogram(binwidth = 1) +
    labs(title = '85 Year Old Spike') +
    theme_ipsum()
```

It looks like there is an incredible amount of older patients. I'm guessing that this is an error.

```{r}
(max_age <- max(df_raw$age, na.rm = TRUE))

df_raw %>% 
  mutate(is_max_age = if_else(age == max_age, 'y','n')) %>% 
  count_pct(is_max_age) %>% 
  select(-pct)
```

Odd, around 10% of patients have the age of exactly 85. I don't know what to do about this yet, because even though it's clearly a spike, it looks like it might be a normal distribution if it continued above. If that's the case, I don't think I should overwrite that info.

```{r}
# I don't know what to do about the crazy 85 thing.
```


Okay, so how many people actually have the issue?

```{r}
df_raw %>% 
  ggplot(aes(result)) +
    geom_bar() +
    labs(title = 'result distribution') +
    theme_ipsum()
```

Very few, it turns out. `r df_raw %>% count_pct(result) %>% .[2,4] %>% pull` to be specific.



Let's run through my (likely very unhelpful) chisq test across all of the variables.

```{r}
chisq_pvalues <- df_raw %>%
  select(one_of(binary_vars)) %>%
  map(table, factor(df_raw$result)) %>%
  map(chisq.test) %>%
  map_dbl("p.value") %>%                   # extract p-value from output
  broom::tidy() %>%                        # convert from a list to a dataframe
  mutate(p_value = round(x,4)) %>%
  select(-x)
```


```{r}
df_raw %>% 
  filter(result == 'yes') %>% 
  group_by(result) %>% 
  summarise_at(binary_vars, sum) %>% 
  select(-result) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column() %>%
  rename(positive_count = V1) %>% 
  arrange(desc(positive_count))
```

In this section, I try to see if there is a bias towards certain kinds of a permutations. I'm not super happy with this code. 

```{r}
# 
# all of this is garbage code
# 

suffixes_plus <- c('Historic', 'Negated', 'Uncertain', 'Surgical', 'Basic')

# I should really just make a function for this
flagged_binary_vars <- binary_vars %>% 
  as_tibble() %>% 
  mutate(permutation = '',
         permutation = if_else(str_detect(value, 'Historic'), 'Historic', coalesce(permutation, '')), 
         permutation = if_else(str_detect(value, 'Negated'), 'Negated', coalesce(permutation, '')),
         permutation = if_else(str_detect(value, 'Uncertain'), 'Uncertain', coalesce(permutation, '')),
         permutation = if_else(str_detect(value, 'Surgical'), 'Surgical', coalesce(permutation, '')),
         permutation = if_else(permutation == '', 'Basic', permutation)
  )

sum_permutation_type <- function(suf, df = df_raw) {
  selected_col <- flagged_binary_vars %>% 
    filter(permutation == suf) %>% 
    pull(value)
  df %>% 
    select(one_of(selected_col)) %>% 
    rowSums() %>% 
    sum()
}

total_permutations <- map_dbl(suffixes_plus, sum_permutation_type) %>% 
                        tibble(count = .,
                               permutation = suffixes_plus)
total_permutations %>% 
  mutate(pct = round(count / sum(count),4)) %>% 
  arrange(desc(pct))
  
yes_permutations <- map_dbl(suffixes_plus, sum_permutation_type, df = filter(df_raw, result == 'yes')) %>% 
                      tibble(count = .,
                             permutation = suffixes_plus) 
yes_permutations %>% 
  mutate(pct = round(count / sum(count),4)) %>% 
  arrange(desc(pct))
```

It looks like there isn't a signficicant difference between the distributions of permutation field type.


# data cleaning

```{r filling_nas}
# gender == 2 has no positive outcomes. I'll just remove them from the analysis
# df_raw %>% 
#   filter(gender == 2,
#          result == 'yes') %>% 
#   nrow

median_age <- median(df_raw$age, na.rm = TRUE)
median_calcium <- median(df_raw$Calcium, na.rm = TRUE)

df <- df_raw %>%
  replace_na(list(age = median_age, Calcium = median_calcium)) %>% 
  filter(gender != 2) %>% 
  mutate_at(binary_vars, as.factor) %>% 
  mutate_at(c('gender', 'result'), as.factor) %>% 
  select(-visitId)


# train <- data[1:800,]
# test <- data[801:889,]

# what should the p be? how many times?


# library(caret)


# df_cv <- df %>% 
#   select(-Doors, -Chevy, -sedan) %>%
#   crossv_kfold(10)
```

There are a lot of variables. A number of the factored variables only have one factor level. I get rid of them here.

I'm making a function here to make my life easier. I'd like to be able to quickly change the partition and not worry about automatically removing factors that won't have any impact.

```{r}
train_test_reset <- function(partition, df) {

  set.seed(1991)
  trainIndex <- caret::createDataPartition(df$result, p = partition, 
                                           list = FALSE, 
                                           times = 1)
  
  train_raw <- df[ trainIndex,]
  test_raw  <- df[-trainIndex,]
  
  # variables with more than one 
  usable_factors <- train_raw %>% 
    select_if(is.factor) %>% 
    map_dbl(~length(levels(.))) %>% 
    enframe() %>% 
    filter(value > 1) %>% 
    pull(name)
  
  # used for getting the other non-factor variables 
  not_factor <- function(x) {!is.factor(x)}
  
  # bringing it all together
  model_variables <- train_raw %>% 
    select_if(not_factor) %>% 
    names %>% 
    c(.,usable_factors)
  
  train <- train_raw %>%
    dplyr::select(one_of(usable_factors))
    
  output <- list(train = train, test = test_raw)
  return(output)
}
```

I'm also evaluating the model in the same way, so I'll make a function for that too.
```{r}
# small function to evaluate the model
evaluate_model <- function(test, prediction) {
  test$prediction <- prediction$class
  
  test %>% 
    mutate(check = (result == prediction)) %>% 
    filter(result == 'yes') %>% 
    count_pct(check)
}
```


# Modeling

## First approach is a basic Naive Bayes model
```{r}
# train a naive bayes model
# model <- train(result~., data=train, method="nb")

train_test <- train_test_reset(.1, df)
train <- train_test$train
test <- train_test$test

library(klaR)
set.seed(1991)
model_nb_1 <- NaiveBayes(result~., data=train)

summary(model_nb_1)
# print(model_nb_1)
prediction_nb_1 <- predict(model_nb_1, test)

evaluate_model(test, prediction_nb_1)

# first run got 44.7%

# # make predictions
# x_test <- data_test[,1:4]
# y_test <- data_test[,5]
# predictions <- predict(model, x_test)
# # summarize results
# confusionMatrix(predictions$class, y_test)
```

## Second approach NB "Best Variables"

So, that first model could have been better. Let's take a closer look at the variables. Here, I've grabbed the variables 10 variables that have the highest percentage of being a 1 for yes. I'm pretty sure this is a bad way to do things.

```{r}
# using the model output from the last model I get a bunch of outputs. I organize them here
(top_by_table <- model_nb_1$tables %>% 
  map_dbl(~.[2,2]) %>% 
  enframe() %>% 
  arrange(desc(value)))

features_nb_2 <- top_by_table %>% 
  filter(name %in% c('gender', binary_vars_unique)) %>% 
  slice(1:10) %>% 
  pull(name)

# When I was tired, I tried to compare conditions and see their relative permutations. I was getting odd results
# df %>% 
#   filter(result == 'yes') %>% 
#   mutate_if(is.factor, as.numeric) %>% 
#   mutate(chk = if_else((C0416980 + C0416980Historic) == 2,1,0),
#          or_chk = if_else((C0416980  + C0416980Historic) %in% c(1,2) ,1,0)) %>%
#   summarise_at(c('C0416980', 'C0416980Historic', 'chk', 'or_chk'), sum)

```


```{r}

train_test <- train_test_reset(.1)

train <- train_test$train %>% 
  dplyr::select(one_of('result',features_nb_2))

test <- train_test$test
  
set.seed(1991)  
model_nb_2 <- NaiveBayes(result~., data=train)
prediction_nb_2 <- predict(model_nb_2, test)

evaluate_model(test, prediction_nb_1)
# this run got 21.1%

```

Woah, that model was way worse. It only got 21.1% accuracy.

## Third Approach: NB Combine variables

```{r}
factor_to_numeric <- function(x) as.numeric(as.character(x))

# create list of dfs for each condition
row_summed_condtions <- 
  map(binary_vars_unique, 
        ~df_unfactored_binary[, str_detect(., colnames(df_unfactored_binary))]
      ) %>%
  map(~rowSums(.))

names(row_summed_condtions) <- str_c(binary_vars_unique,"_combined") # name lists for conversion into df
  
df_combined <- row_summed_condtions %>% 
  as_tibble() %>% 
  map(~if_else(. > 0, 1, 0)) %>% 
  map(as.factor) %>%
  bind_cols(df[c('age', 'gender', 'result', 'Calcium','xrayTestService','petScanService','ctTestService')])

```

```{r}
train_test <- train_test_reset(.1, df_combined)

train <- train_test$train
test <- train_test$test

set.seed(1991)
model_nb_3 <- NaiveBayes(result~., data=train)
prediction_nb_3 <- predict(model_nb_3, test)

evaluate_model(test, prediction_nb_3)
# 25.3%
```



## Fourth Approach: kitchen sink random forest

It's clear that shooting into the dark hasn't been particularly productive. I'll try a random forest to identify what the most important variables are. With those variables, maybe I can come up with a better prediction.

```{r}
library(randomForest)

train_test <- train_test_reset(.5, df)

train <- train_test$train
test <- train_test$test

set.seed(1991)
model_rf_4 <- randomForest(result~., data = train)

prediction_rf_4 <- predict(model_rf_4, test)

prediction_rf_4 %>% 
  as_tibble() %>% 
  rename(prediction = value) %>% 
  bind_cols(., test) %>% 
  mutate(check = (result == prediction)) %>% 
  filter(result == 'yes') %>% 
  count_pct(check)

```
Zero results when run with .1???? That seems just wrong
Zero results when run with .5???? That's just rude.

```{r}
# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() + 
  theme_few()
```

## fifth Approach: do more weirds stuff to the data

Here I identify rid of all variables that are 'unpopular' in the result = yes subset of the DF. I have one list that is all but 0 and one that is less than 6.
```{r}
df_yes <- df %>% 
  filter(result == 'yes')


var_popularity <- function(df, var_id) {
  df %>% 
    select(one_of(var_id)) %>%
    mutate_all(factor_to_numeric) %>% 
    summarise_all(sum) %>% 
    t %>%
    as.data.frame %>% 
    rownames_to_column() %>% 
    rename(total_value = V1) %>%
    arrange(desc(total_value)) %>% 
    filter(total_value != 0)
}

binary_vars_pos_df <- var_popularity(df_yes, binary_vars)

binary_vars_pos_df %>% 
  ggplot(aes(total_value)) + 
    geom_histogram()

binary_vars_pos_df_6 <- binary_vars_pos %>% 
  filter(total_value > 5)

binary_vars_pos <- pull(binary_vars_pos_df, rowname)
binary_vars_pos_6 <- pull(binary_vars_pos_df_6, rowname)

```

Here I combine current and historic.

```{r}
unique_df <- df %>% 
  select(one_of(binary_vars_unique))

historical_df <- df %>% 
  select(ends_with("Historic"))

has_had_df <- bind_cols(unique_df, historical_df) %>%
  mutate_all(factor_to_numeric)

combine_historical <- function(condtion_code) {
  has_had_df %>%   
    select(contains(condtion_code)) %>% 
    rowSums(.) %>% 
    as_tibble() %>% 
    transmute(value = if_else(value == 0, 0, 1 )) %>% 
    pull
}

has_had_df <- map(binary_vars_unique, combine_historical)
names(has_had_df) <- binary_vars_unique

has_had_df <- as_tibble(bort) %>% 
  mutate_all(as.factor)
names(has_had_df) <- str_c(binary_vars_unique,'_has_had')

has_had_df <- df %>% 
  select(-contains('Historic'),
         -one_of(binary_vars_unique)) %>% 
  bind_cols(., has_had_df)
```

Now I prepare the data to only keep the above 5 variables

```{r}
binary_vars_has_had <- c(binary_vars[!(binary_vars %in% binary_vars_unique)], str_c(binary_vars_unique,'_has_had'))
binary_vars_has_had <- binary_vars_has_had[!str_detect(binary_vars_has_had,'Historic')]

binary_vars_pos_has_had <- has_had_df %>% 
  filter(result == 'yes') %>% 
  var_popularity(., binary_vars_has_had)

binary_vars_pos_df_6_has_had <- binary_vars_pos_has_had %>% 
  filter(total_value > 5)

binary_vars_pos_6_has_had <- pull(binary_vars_pos_df_6_has_had, rowname)


```



```{r}
has_had_df_popular <- has_had_df %>% 
  select(one_of(binary_vars_pos_6_has_had), age, gender, result, xrayTestService, petScanService)
  

train_test <- train_test_reset(.1, has_had_df_popular)
train <- train_test$train
test <- train_test$test

library(klaR)
set.seed(1991)
model_nb_5 <- NaiveBayes(result~., data=train)

summary(model_nb_5)
# print(model_nb_1)
prediction_nb_5 <- predict(model_nb_5, test)

evaluate_model(test, prediction_nb_5)
```



## sixth Approach: nural net kitchen sink

Okay, that one did even worse. Maybe I'll try a neural network before I start picking off variables again.

```{r}
library(nnet)

train_test <- train_test_reset(.1, df)

train <- train_test$train
test <- train_test$test

set.seed(1991)
model_nn_6 <- nnet(result~., data=train, size=4, decay=0.0001, maxit=500)

# summary(model_nn_6)
# print(model_nn_6)
prediction_nn_6 <- predict(model_nn_6, test)

# evaluate_model(test, prediction_nn_6)
```

## seventh Approach: nural net selected variables

```{r}

train_test <- train_test_reset(.1, has_had_df_popular)

train <- train_test$train
test <- train_test$test

set.seed(1991)
model_nn_7 <- nnet(result~., data=train, size=4, decay=0.0001, maxit=500)

# summary(model_nb_5)
# print(model_nb_1)
prediction_nn_7 <- predict(model_nn_7, test)

# evaluate_model(test, prediction_nn_7)
```



```{r}
# MAKE BALANCED THEN DO THIS
prop.table(table(df$result, df$C0016641)) %>% 
  min


```


# micah advice
balance the training data
randomize!!! 
start with 1:1 
try 1:2 or 1:3
---logit regression
---chisq w/ balanced dataset

# Vishvesh advice
logit regression on everything
remove every variable t val > 1.96
do it again that is the base for the model


# Joe's advice
Logit would be perfect if I knew the relationship generally

naieve bayes -- classic basic, performs well 
machine learning logit model 
neural networks -- super sexy 

# Karl advice
gausian elimination
sum the projections with each of the row vectors

dot product
matrix multiplication
eigen values / vectors 
diagonalize matrix

