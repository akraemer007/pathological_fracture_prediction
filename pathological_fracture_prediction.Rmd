---
title: "Pathological Fracture Prediction"
output: html_notebook
---

The goal of this analysis is to create a model to predict if a patient has a pathologic fracture. A quick Wikipedia search explains:

> A pathologic fracture is a bone fracture caused by disease that led to weakness of the bone structure. This process is most commonly due to osteoporosis, but may also be due to other pathologies such as: cancer, infection, inherited bone disorders, or a bone cyst.

I ran a number of models. The key to getting predictions greater than 66% accuracy was undersampling the training data. In additon to increaing model accuracy, undersampling also significatly reduced the time it took to train my models. 

The following is a step-by-step walk through of how I approached the data, how I cleaned the data, how I prepared the training set, and my models. 


```{r packages, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(dplyr)
library(magrittr)
library(stringr)
# devtools::install_github('hrbrmstr/hrbrmisc')
library(hrbrmisc)
# devtools::install_github('hrbrmstr/hrbrthemes')
library(hrbrthemes)
```

First off, let's import the data.

```{r data_pull, eval=FALSE, include=FALSE}
# I'll figure this out later if I have time
# if (!file.exists("PathologicalFracturesDataSet.csv")) {
  # id <- "0B4W_zSMkDcS9SlRrTXZlLVBGU3M" # google file ID
  # read_csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# }
```

```{r data_import, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
df_raw <- read_csv("PathologicalFracturesDataSet.csv")
```

Before we start to model, it's always important to get a sense of the data. The first thing I noticed is that this is quite a large dataset. It looks like there are `r nrow(df_raw)` rows, one for each patient, and `r length(names(df_raw))` variables. 

That's quite a bit of variables, let's take a closer look:

```{r}
df_raw %>% 
  select(1:7, Calcium, 8:12) %>%
  slice(1:5) %>% 
  glimpse
```

Three of the variables are straight-forward: `visitID`, `age`, and `gender`. There are four tests in the data: `xrayTestService`, `petScanService`, `ctTestService`, `Calcium`, which are all continuous variables. The remaining variables are binary variables. There are 70 conditions in the dataset, each with their own code, such as `C1882062`, which is "Neoplastic Syndrome". Each of these conditions have 5 permutations: `currently has`, `Historic`, `Negated`, `uncertian`, and `Surgical`.

```{r, eval=FALSE, include=FALSE}
# looks like 
# df_raw %>% 
#   mutate(bort = str_sub(visitId,-1,-1),
#          yes_check = if_else(result == 'yes', 1, 0)) %>% 
#   group_by(bort) %>% 
#   summarise(idonteven = sum(yes_check) / sum(n()))
```

## Checking the data

### Null values

```{r, include=FALSE}
row_count_no_na <- nrow(drop_na(df_raw))
```

The existance of NA values in a dataset is the quickest way to cause confusion and break models, so it's usually the first thing I check for. There are `r nrow(df_raw) - row_count_no_na` rows with NA values. That's very few relative to our total number of records, luckily it seems we have a pretty clean dataset.

Now let's quickly see which variables have null records with this loop:
```{r}
df_raw %>% 
  map_lgl(anyNA) %>% # loop all columns with an anyNA check
  .[. == TRUE]
```
Looks like just age and Calcium have NAs, which are both are continuous variables. There are a number of ways to deal with variables, but let's take a closer look at them first.

First, let's take a look the age distribution before we simply take the median.
```{r}
df_raw %>% 
  ggplot(aes(age)) +
    geom_histogram(binwidth = 1) +
    labs(title = '85 Year Old Spike') +
    theme_ipsum()
```

```{r, eval=FALSE, include=FALSE}
max_age <- max(df_raw$age, na.rm = TRUE)

df_raw %>% 
  mutate(is_max_age = if_else(age == max_age, 'y','n')) %>% 
  count(is_max_age) %>%
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))
```

About 10% of patients have the age of exactly 85. I don't know what to do about this yet, because even though it's clearly an outlier spike, it looks like it might be a normal distribution if it continued above. If that's the case, I don't think I should overwrite that info. We can come back to this later.

I looked at the calcuim variable and didn't find have anything particularly interesting.



```{r, eval=FALSE, include=FALSE}
# df_raw %>% 
#   ggplot(aes(Calcium, fill = result)) +
#     geom_histogram(binwidth = 1) +
#     labs(title = 'Calcium') +
#     theme_ipsum()
```

Since the population is low, I'll keep this simple. I'll replace any NAs with the median value of the variable.

```{r}
median_age <- median(df_raw$age, na.rm = TRUE)
median_calcium <- median(df_raw$Calcium, na.rm = TRUE)

df <- df_raw %>%
  replace_na(list(age = median_age, Calcium = median_calcium))
```

```{r, eval=FALSE, include=FALSE}
# duplicate visitID check. no dups
df %>% 
  count(visitId) %>% 
  filter(n > 1) %>% 
  nrow

# I checked the visit ID. I found that dup ids with different end letters seem to be different people. Like, they have different genders
df %>%
  transmute(visitId = str_sub(visitId, 1, -2)) %>%
  count(visitId) %>%
  arrange(desc(visitId)) %>%
  filter(n > 1)
```

What numbers are in the variables? Are they all zeroes and ones? I'm removing the fields I can already tell are not zeroes and ones. 

```{r}
# Just removed gender from binary variables
non_binary_vars <- c('visitId', 'age', 'result', 'xrayTestService', 'petScanService', 'ctTestService', 'Calcium')

binary_vars <- names(df)[!(names(df) %in% non_binary_vars)]

max_cols <- df %>% 
  select(-one_of(non_binary_vars), gender) %>% 
  summarise_all(max) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column() %>% 
  rename(max_value = V1)

max_cols %>% 
  arrange(desc(max_value)) %>% 
  filter(max_value > 1)
```


As expected, most of the variables are binary. However, it is surprising to see that gender has some values that are equal to 2. 

```{r}
df %>% 
  count(gender) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent)) %>% 
  arrange(gender)
```

Taking a closer look, it seems that there are only 22 instances where `gender = 2`. Since the amount is so low, I'll remove them from the dataset.

```{r, include=FALSE}
df <- filter(df, gender != 2)
```




```{r, include=FALSE}
suffixes <- c('Historic', 'Negated', 'Uncertain', 'Surgical')

length(binary_vars) / 5 # looks like there are the correct number of columns
  
# get unique binary vars 
binary_vars_unique <- binary_vars %>% 
  str_replace('Historic', '') %>% 
  str_replace('Negated', '') %>% 
  str_replace('Uncertain', '') %>% 
  str_replace('Surgical', '') %>% 
  unique

# get the max number of `1s` a variable can have across the different types
var_max_answer_check <- function(var, df = df_raw) {
  df %>% 
  select(starts_with(var)) %>% 
  rowSums %>% 
  max
}

# loop by different variables
map_dbl(binary_vars_unique, var_max_answer_check) %>% 
  as_tibble() %>% 
  count(value) %>% 
  mutate(pct = round(n/ sum(n),4))

# map_dbl(binary_vars_unique, var_max_answer_check, df = filter(df_raw, result == 'yes')) %>% 
#   as_tibble() %>% 
#   count(value) %>% 
#   mutate(pct = round(n/ sum(n),4))
```


```{r, eval=FALSE, include=FALSE}
df %>% 
  ggplot(aes(result, age)) +
    geom_boxplot() +
    labs(title = 'age') +
    theme_ipsum()

df %>% 
  ggplot(aes(result, xrayTestService)) +
    geom_boxplot() +
    labs(title = 'xrayTestService') +
    theme_ipsum()
df %>% 
  count(xrayTestService)

df %>% 
  ggplot(aes(result, petScanService)) +
    geom_boxplot() +
    labs(title = 'petScanService') +
    theme_ipsum()

df %>% 
  ggplot(aes(result, ctTestService)) +
    geom_boxplot() +
    labs(title = 'ctTestService') +
    theme_ipsum()

df %>% 
  filter(result == 'yes') %>% 
  count(Calcium, result) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))
  
  

```

Now that a lot of the general data cleaning has been performed, it's time to ask some basic questions. For instance, how many `result = yes` cases are there in the dataset? 

```{r}
df %>% 
  ggplot(aes(result)) +
    geom_bar() +
    labs(title = 'result distribution') +
    theme_ipsum()
```

The chart above shows that a very small subset of the data has positive instances of pathological fractures. `r df %>% count(result) %>% mutate(percent = n / sum(n), percent = scales::percent(percent)) %>% .[2,3] %>% pull` to be specific.

This model will have to find a needle in a haystack.

```{r, include=FALSE}
df %<>% 
  mutate_at(binary_vars, as.factor) %>% 
  mutate(result = as.factor(result)) %>% 
  dplyr::select(-visitId)
```

# Modeling

The first step in a predictive model is to separate the data between what is going to be trained and what is going to be tested. It's important to do this randomly to avoid accidentally biasing the data. Furthermore, it is important to maintain a balance of your outcome variable between the train and test datasets. I do that here using the famous `caret` package. I first decided to slit the data set into .1 train .9 test, due to the size of the dataset and number of values.

```{r}
train_test <- function(df, partition) {
  set.seed(1991)
  
  trainIndex <- caret::createDataPartition(df$result, p = partition, 
                                           list = FALSE, 
                                           times = 1)
  train_base <- df[ trainIndex,]
  test_base  <- df[-trainIndex,]

  output <- list(train = train_base, test = test_base)
  return(output)
}

train_test <- train_test(df, .1)
train_base_1 <- train_test$train
test_base_1 <- train_test$test

```


There are a lot of variables and many of them only have one factor level. I'll be slicing the data a few times, so I'll create a function to remove these unhelpful variables from the training set. When I first ran this analysis I was most interested in the binary variables, since it was easier to check the non-binary variables on my own.

```{r}
remove_unhelpful_factors <- function(df) {

  # variables with more than one 
  usable_factors <- df %>% 
    select_if(is.factor) %>% 
    map_dbl(~length(levels(.))) %>% 
    enframe() %>% 
    filter(value > 1) %>% 
    pull(name)
  
  # used for getting the other non-factor variables 
  not_factor <- function(x) {!is.factor(x)}
  
  # bringing it all together
  model_variables <- df %>% 
    select_if(not_factor) %>% 
    names %>% 
    c(.,usable_factors)
  
  return(dplyr::select(df, one_of(usable_factors)))
}
```

I'm also evaluating the model in the same way, so I'll make a function for that too.
```{r}
# small function to evaluate the model
evaluate_model <- function(test, prediction) {
  test$prediction <- prediction$class
  
  test %>% 
    mutate(check = (result == prediction)) %>% 
    group_by(result) %>% 
    count(check) %>% 
    mutate(percent = n / sum(n),
           percent = scales::percent(percent))
}
```


# Modeling

## Model 1 Naive Bayes *the kitchen sink*

A lot of my preliminary data exploration was somewhat dissapointing. Outside of what is shown in this report, I explored a number of the variables looking for a relationship with `result = yes`, but nothing stood out. As such, I decided to try a model with all of the binary variables to see what would happen. I used naive bayes as it is generally a dependable and easy to use model.

```{r, message=FALSE, warning=FALSE}
train <- remove_unhelpful_factors(train_base_1)
test <- test_base_1

library(klaR)
set.seed(1991)
model_nb_1 <- NaiveBayes(result~., data=train)

summary(model_nb_1)
# print(model_nb_1)
prediction_nb_1 <- predict(model_nb_1, test)

evaluate_model(test, prediction_nb_1)

```

Despite only running on 10% of the data. This model took about a half-hour to run. 44.7% of true results were predicted. For a first pass, I was suprised by how accurate the model was.

After running this model, I tried a number of other approaches:

- I only kept the features which correlated most strongly to yes and no (21.1% accuracy)
- I ran a model that only kept conditions that were current (25.3% accuracy)
- I combined the condition variables in a number of ways (28.3%, 26% accuracy)
- I tried a random forest (0% accurate)

I ran the random forest when I left Madison to go to my college reunion in Appleton. The model ran for the entire two hour drive in my passenger seat. When the model clearly didn't work, I decided to try a new approach. Feature seletion and model experimentation were getting me nowhere.

## The break through

The presence of a pathological fracture is a rare event in this dataset. I figured that the disproportionate number of negatives must be bloating or confusing the model. I researched unbalanced datasets and found a number of different ways to apprach this problem. The most straight-forward was undersampling. Undersampling is when you randomly remove data from your training set to even the number of positive and negative cases.

I created a function to quickly balance my training set:

```{r}
balance_df <- function(df, ratio = 1) {
  
  train_pos <- filter(df, result == 'yes')
  
  train_neg <- filter(df, result == 'no')
  
  train_neg <- train_neg[sample(1:nrow(train_neg), ratio*nrow(train_pos), replace=FALSE),]
  
  return(bind_rows(train_pos, train_neg))
}

```

```{r}
train_test <- train_test(df, .5)
train_base_5 <- train_test$train
test_base_5 <- train_test$test

train <- balance_df(train_base_5)
train <- remove_unhelpful_factors(train)

set.seed(1991)
model <- NaiveBayes(result~., data=train)

prediction_nb_2 <- predict(model, test_base_5)

evaluate_model(train_test$test, prediction)
```

The under sampling method worked quite well. The model predicts true-positives 70.1% of the time, and has false negatives 11.6% of the time. This model took several minutes to run, so I decided to try running a logit model to see a different approach would help with performance and accuracy.

The logit model is a bit more sensitive than naive bayes and has some different outputs so the code is a bit different.

```{r, message=FALSE, warning=FALSE}
valid_cols <- train %>% 
  summarise_all(n_distinct) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column %>% 
  rename(value = V1) %>% 
  filter(value > 1) %>% 
  pull(rowname)

train %<>% 
  dplyr::select(one_of(valid_cols))

set.seed(1991)
model_l_3 <- glm(result~., family=binomial(link='logit'), 
                 data=train)

prediction_l_3 <- predict(model_l_8, test_base_5, type='response')

test_base_5$prediction <- prediction_l_3

test_base_5 %>% 
  mutate(prediction_yn = if_else(prediction <.5, 'no', 'yes'),
         check = (result == prediction_yn)) %>% 
  group_by(result) %>% 
  count(check) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))

```

Here we see the true positive performance is slightly improved, whereas the false negatives have also increased. Relative to naive bayes, logit is nice since it gives probabilities instead of raw outputs. Using a ROC curve, we can see how setting the probability threshold will impact our type 1 and type 2 errors.

```{r, include=FALSE}
library(plotROC)
test_base_5 %>%
  mutate(result = if_else(result == 'no', 0, 1)) %>% 
  ggplot(aes(d = result, m = prediction)) +
    geom_roc()
```




