---
title: "Pathological Fracture Prediction"
author: Andrew Kraemer
date: 2017-06-20
output: html_notebook
---

# Introduction

I was asked to build a predictive model for pathological fractures. I ran a number of models before getting above 50% accuracy. The key to getting predictions greater than 66% accuracy was under-sampling the training data. In addition to increasing model accuracy, under-sampling also significantly reduced the time it took to train my models. There are a number of approaches, models, and variable manipulations that I did not try here that can be used in future analyses.

The following is a step-by-step walk through of how I approached, cleaned, prepared, and modeled the data. What is shown is a subset of my code. The full analysis can be found on my [github page](https://github.com/kidman007/pathological_fracture_prediction/blob/full_analysis/pathological_fracture_prediction.Rmd). 


```{r packages, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
library(tidyverse)
library(magrittr)
library(stringr)
# devtools::install_github('hrbrmstr/hrbrthemes')
library(hrbrthemes)
```

# Data First Look

First off, let's import the data.

```{r data_pull, eval=FALSE, include=FALSE}
# I'll figure this out later if I have time
# if (!file.exists("PathologicalFracturesDataSet.csv")) {
  # id <- "0B4W_zSMkDcS9SlRrTXZlLVBGU3M" # google file ID
  # read_csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
# }
```

```{r data_import, echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
df_raw <- read_csv("PathologicalFracturesDataSet.csv")
```

Before we start to model, it's always important to get a sense of the data. The first thing I noticed is that this is quite a large data set. There are `r nrow(df_raw)` rows, one for each patient, and `r length(names(df_raw))` variables. 

That's a lot of information; let's take a look at the first five records:

```{r}
df_raw %>% 
  dplyr::select(1:7, Calcium, 8:12) %>%
  slice(1:5) %>% 
  glimpse
```

Three of the variables are straight-forward: `visitID`, `age`, and `gender`. There are four tests in the data: `xrayTestService`, `petScanService`, `ctTestService`, and `Calcium`. All tests are continuous variables, except for calcium, which seems to be a factor. Fractures deal with bones. I've been told that the calcium in milk builds strong bones my entire life. Perhaps this is an important variable.

The remaining variables are binary variables. There are 70 conditions in the data set, each with their own code, such as `C1882062` show above, which is "Neoplastic Syndrome." Each of these conditions have 5 permutations: `Currently_Has` (unlabeled), `Historic`, `Negated`, `Uncertian`, and `Surgical`.

```{r, eval=FALSE, include=FALSE}
# looks like 
# df_raw %>% 
#   mutate(bort = str_sub(visitId,-1,-1),
#          yes_check = if_else(result == 'yes', 1, 0)) %>% 
#   group_by(bort) %>% 
#   summarise(idonteven = sum(yes_check) / sum(n()))
```

# Checking the Data

### Null values

```{r, include=FALSE}
row_count_no_na <- nrow(drop_na(df_raw))
```

The existence of NA values in a data set is the quickest way to cause confusion and break models, so it's usually the first thing I check for. There are `r nrow(df_raw) - row_count_no_na` rows with NA values. That's very few relative to our total number of records, so it seems we have a pretty clean data set.

Now let's quickly see which variables have null records with this loop:
```{r}
df_raw %>% 
  map_lgl(anyNA) %>% # loop all columns with an anyNA check
  .[. == TRUE]
```
Looks like just age and Calcium have NAs. There are a number of ways to deal with variables, but it is important to understand the information before we pick an approach.

#### Age

First, let's take a look the age distribution median.
```{r}
df_raw %>% 
  ggplot(aes(age)) +
    geom_histogram(binwidth = 1) +
    labs(title = '85 Year Old Spike') +
    theme_ipsum()
```

```{r, eval=FALSE, include=FALSE}
max_age <- max(df_raw$age, na.rm = TRUE)

df_raw %>% 
  mutate(is_max_age = if_else(age == max_age, 'y','n')) %>% 
  count(is_max_age) %>%
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))
```

This spike caught my eye. About 10% of patients have the age of exactly 85. I decided not to do anything about the 85 year old patient because it looks like the spike would match the normal distribution if it continued above. I'll replace the NAs with the median.

#### Calcium

```{r, echo=TRUE}
df_raw %>%
  count(Calcium) %>% 
    mutate(percent = n / sum(n),
           percent = scales::percent(percent))
```

It looks like the distribution between the calcium amounts is even.

#### Fill NAs

```{r}
median_age <- median(df_raw$age, na.rm = TRUE)

df <- df_raw %>%
  replace_na(list(age = median_age)) %>% 
  fill(Calcium) # take preceding value to fill it in
```

### Check binary variables

```{r, eval=FALSE, include=FALSE}
# duplicate visitID check. no dups
df %>% 
  count(visitId) %>% 
  filter(n > 1) %>% 
  nrow

# I checked the visit ID. I found that dup ids with different end letters seem to be different people. Like, they have different genders
df %>%
  transmute(visitId = str_sub(visitId, 1, -2)) %>%
  count(visitId) %>%
  arrange(desc(visitId)) %>%
  filter(n > 1)
```

The remaining ~350 variables seem to be binary, but I checked to make sure that this is true. I check here. 

```{r}
# Just removed gender from binary variables
non_binary_vars <- c('visitId', 'age', 'result', 'xrayTestService', 'petScanService', 'ctTestService', 'Calcium')

binary_vars <- names(df)[!(names(df) %in% non_binary_vars)]

max_cols <- df %>% 
  dplyr::select(-one_of(non_binary_vars), gender) %>% 
  summarise_all(max) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column() %>% 
  rename(max_value = V1)

max_cols %>% 
  arrange(desc(max_value)) %>% 
  filter(max_value > 1)
```


As expected, most of the variables are binary. However, it is surprising to see that gender has some values that are equal to 2. 

#### Gender

```{r}
df %>% 
  count(gender) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent)) %>% 
  arrange(gender)
```

Taking a closer look, it seems that there are only 22 instances where `gender = 2`. Since the amount is so low, I'll remove them from the data set.

```{r, echo=TRUE}
df <- filter(df, gender != 2)
```

### Exploration

```{r, include=FALSE}
suffixes <- c('Historic', 'Negated', 'Uncertain', 'Surgical')

length(binary_vars) / 5 # looks like there are the correct number of columns
  
# get unique binary vars 
binary_vars_unique <- binary_vars %>% 
  str_replace('Historic', '') %>% 
  str_replace('Negated', '') %>% 
  str_replace('Uncertain', '') %>% 
  str_replace('Surgical', '') %>% 
  unique

# get the max number of `1s` a variable can have across the different types
var_max_answer_check <- function(var, df = df_raw) {
  df %>% 
  dplyr::select(starts_with(var)) %>% 
  rowSums %>% 
  max
}

# loop by different variables
map_dbl(binary_vars_unique, var_max_answer_check) %>% 
  as_tibble() %>% 
  count(value) %>% 
  mutate(pct = round(n/ sum(n),4))

# map_dbl(binary_vars_unique, var_max_answer_check, df = filter(df_raw, result == 'yes')) %>% 
#   as_tibble() %>% 
#   count(value) %>% 
#   mutate(pct = round(n/ sum(n),4))
```


```{r, eval=FALSE, include=FALSE}
df %>% 
  ggplot(aes(result, age)) +
    geom_boxplot() +
    labs(title = 'age') +
    theme_ipsum()

df %>% 
  ggplot(aes(result, xrayTestService)) +
    geom_boxplot() +
    labs(title = 'xrayTestService') +
    theme_ipsum()
df %>% 
  count(xrayTestService)

df %>% 
  ggplot(aes(result, petScanService)) +
    geom_boxplot() +
    labs(title = 'petScanService') +
    theme_ipsum()

df %>% 
  ggplot(aes(result, ctTestService)) +
    geom_boxplot() +
    labs(title = 'ctTestService') +
    theme_ipsum()

df %>% 
  filter(result == 'yes') %>% 
  count(Calcium, result) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))
```

Now that a lot of the general data cleaning has been performed, it's time to ask some basic questions. For instance, how many `result = yes` cases are there in the data set? 

```{r}
df %>% 
  ggplot(aes(result)) +
    geom_bar() +
    labs(title = 'result distribution') +
    theme_ipsum()
```

The chart above shows that a very small subset of the data has positive instances of pathological fractures. `r df %>% count(result) %>% mutate(percent = n / sum(n), percent = scales::percent(percent)) %>% .[2,3] %>% pull` to be specific.

This model will have to find a needle in a haystack.

```{r, include=FALSE}
df %<>% 
  mutate_at(binary_vars, as.factor) %>% 
  mutate(result = as.factor(result))
```

# Training Data Setup

The first step in a predictive model is to separate the data between what is going to be trained and what is going to be tested. It's important to do this randomly to avoid accidentally biasing the data. Furthermore, it is important to maintain a balance of your outcome variable between the train and test data sets. I do that here using the famous `caret` package. I first decided to slit the data set into .1 train .9 test, due to the size of the data set and number of values.

```{r}
train_test <- function(df, partition) {
  set.seed(1991)
  
  trainIndex <- caret::createDataPartition(df$result, p = partition, 
                                           list = FALSE, 
                                           times = 1)
  train_base <- df[ trainIndex,]
  test_base  <- df[-trainIndex,]

  output <- list(train = train_base, test = test_base)
  return(output)
}

test_train <- train_test(df, .1)
train_base_1 <- test_train$train
test_base_1 <- test_train$test

```

There are a lot of variables and many of them only have one factor level. I'll be slicing the data a few times, so I'll create a function to remove these unhelpful variables from the training set. When I first ran this analysis I was most interested in the binary variables.

```{r}
remove_unhelpful_factors <- function(df) {

  # variables with more than one 
  usable_factors <- df %>% 
    select_if(is.factor) %>% 
    map_dbl(~length(levels(.))) %>% 
    enframe() %>% 
    filter(value > 1) %>% 
    pull(name)
  
  # used for getting the other non-factor variables 
  not_factor <- function(x) {!is.factor(x)}
  
  # bringing it all together
  model_variables <- df %>% 
    select_if(not_factor) %>% 
    names %>% 
    c(.,usable_factors)
  
  return(dplyr::select(df, one_of(usable_factors)))
}
```

Since I'm most interested in finding true positives, I made this function to quickly check my models.
```{r}
# small function to evaluate the model
evaluate_model <- function(test, prediction) {
  test$prediction <- prediction$class
  
  test %>% 
    mutate(check = (result == prediction)) %>% 
    group_by(result) %>% 
    count(check) %>% 
    mutate(percent = n / sum(n),
           percent = scales::percent(percent))
}
```


# Modeling

## Model 1: Naive Bayes *the kitchen sink*

A lot of my preliminary data exploration were somewhat disappointing. Outside of what is shown in this report, I explored a number of the variables looking for a relationship with `result = yes`, but nothing stood out. As such, I decided to try a model with all of the binary variables to see what would happen. I used Naive Bayes as it is generally a performant and easy to use model.

```{r, message=FALSE, warning=FALSE}
train <- remove_unhelpful_factors(train_base_1)
test <- test_base_1

set.seed(1991)
model_nb_1 <- klaR::NaiveBayes(result~., data=train)

prediction_nb_1 <- predict(model_nb_1, test)

evaluate_model(test, prediction_nb_1)

```

Despite only running on 10% of the data. This model took about a half-hour to run. 44.7% of true results were predicted. For a first pass, I was surprised by how accurate the model was.

After running this model, I tried a number of other approaches:

- I only kept the features which correlated most strongly to yes and no (21.1% accuracy)
- I ran a model that only kept conditions that were current (25.3% accuracy)
- I combined the condition variables in a number of ways (28.3%, 26% accuracy)
- I tried a random forest (0% accurate)

I ran the random forest when I left Madison to go to my college reunion in Appleton. The model ran for the entire two hour drive in my passenger seat. When the model clearly didn't work, I decided to try a new approach. Feature selection and model experimentation were getting me nowhere.

## Model 2: The Break Through

The presence of a pathological fracture is a rare event in this data set. I figured that the disproportionate number of negatives must be bloating or confusing the model. I researched unbalanced data sets and found a number of different ways to approach this problem. The most straight-forward was under-sampling. Under-sampling is when you randomly remove data from your training set to even the number of positive and negative cases.

I created a function to quickly balance my training set:

```{r}
balance_df <- function(df, ratio = 1) {
  
  train_pos <- filter(df, result == 'yes')
  
  train_neg <- filter(df, result == 'no')
  
  train_neg <- train_neg[sample(1:nrow(train_neg), ratio*nrow(train_pos), replace=FALSE),]
  
  return(bind_rows(train_pos, train_neg))
}

```

```{r}
test_train <- train_test(df, .5)
train_base_5 <- test_train$train
test_base_5 <- test_train$test

train <- balance_df(train_base_5)
train <- remove_unhelpful_factors(train)

set.seed(1991)
model <- klaR::NaiveBayes(result~., data=train)

prediction_nb_2 <- predict(model, test_base_5)

evaluate_model(test_base_5, prediction_nb_2)
```

The under sampling method worked quite well. The model predicts true-positives 70.1% of the time, and has false negatives 11.6% of the time. This model meets the requirements of the model.

## Model 3: Logit Experimentation

Yes, previous model met the requirements, but Naive Bayes can be a bit of a black box. A logit model is a bit easier to interperet, and, in this case, has significantly better performance. 

A logit model is a bit more sensitive to the input data than Naive Bayes and has some different outputs so the code to run and interpret differs a bit from Naive Bayes.

```{r, message=FALSE, warning=FALSE}
valid_cols <- train %>% 
  summarise_all(n_distinct) %>% 
  t %>%
  as.data.frame %>% 
  rownames_to_column %>% 
  rename(value = V1) %>% 
  filter(value > 1) %>% 
  pull(rowname)

train %<>% 
  dplyr::select(one_of(valid_cols))

set.seed(1991)
model_l_3 <- glm(result~., family=binomial(link='logit'), 
                 data=train)

prediction_l_3 <- predict(model_l_8, test_base_5, type='response')

test_base_5$prediction <- prediction_l_3

test_base_5 %>% 
  mutate(prediction_yn = if_else(prediction <.5, 'no', 'yes'),
         check = (result == prediction_yn)) %>% 
  group_by(result) %>% 
  count(check) %>% 
  mutate(percent = n / sum(n),
         percent = scales::percent(percent))

```

Here we see the true positive performance is slightly improved, whereas the false negatives have also increased. Relative to Naive Bayes, logit is nice since it gives probabilities instead of raw outputs. Using a ROC curve, we can see how setting the probability threshold will impact our type 1 and type 2 errors. Not knowing the preference of type 1 and type 2 errors, I decided to keep the threshold at `.5`.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(plotROC)
test_base_5 %>%
  mutate(result = if_else(result == 'no', 0, 1)) %>% 
  ggplot(aes(d = result, m = prediction)) +
    geom_roc()
```
# Conclusion and next steps: 

Builing this model was a great learning experience. New datasets always produce new chalenges. I was surprised that manipulating the training set -- rather than focusing on feature selection -- ended up being the most effective way of improving accuracy. 

While this model meets the specified requirements, there are a number of approaches I did not have time to investigate:

- Using the letter of the visitID as its own variable.
- Getting to know the conditions better. 
- Over sampling
- Cross fold validation
- Create easy, dynamic functions for easier feature selection


```{r, echo=TRUE}
test_base_5 %>%
  mutate(prediction_yn = if_else(prediction <.5, 'no', 'yes'),
         check = (result == prediction_yn)) %>% 
  dplyr::select(visitId, result, prediction_yn, check) %>% 
  write_csv('final_model_results.csv')
```

